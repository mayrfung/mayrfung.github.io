<!DOCTYPE html>
<html lang="en">
<head>
	<div class="navbar">
		<a href="./index.html"><font size="+1">Home</font></a>
		<a href="./group.html"><font size="+1">Group</font></a>
        <a href="./publication.html"><font size="+1">Publications</font></a>
		<a href="./teach.html"><font size="+1">Talks</font></a>
		<a href="./awards.html"><font size="+1">Service and Awards</font></a>
		<a href="./alumni.html"><font size="+1">Alumni</font></a>
		<a href="./misc.html"><font size="+1">Misc</font></a>
	</div>
    <title>Publications</title>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body style="color: rgb(0, 0, 0, 0.938);margin:0;padding:0">
<div id="wrapper">
    <div id="content-wrap">
        <h1><a name="publications">Selected Highlights</h1>
	    <a style="color:#808080;">Please check out </a><a href="https://scholar.google.com/citations?user=eUae2K0AAAAJ" style="color:#808080;">here</a><a style="color:#808080;"> for the full publication list.</a>
		<ol>
            <font size="3">
			<li>
				<b>VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues</b>
				<br><i>ACL'25</i>. [<a href="https://arxiv.org/pdf/2502.12084" style="color:#9B0145;">arXiv</a>][<a href="" style="color:#9B0145;">code</a>]
			</li>
		        <br>
		    	<li>
				<b>MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration</b>
				<br><i>ACL'25</i>. [<a href="https://arxiv.org/pdf/2505.23224" style="color:#9B0145;">arXiv</a>] [<a href="" style="color:#9B0145;">code</a>]
			</li>
		        <br>
		    	<li>
				<b>Webwatcher: Breaking new frontiers of vision-language deep research agent</b>
				<br><i>ICLR'26</i>. [<a href="" style="color:#9B0145;">arXiv</a>] [<a href="https://github.com/Alibaba-NLP/DeepResearch" style="color:#9B0145;">code</a>]
			</li>	
		        <br>
		    	<li>
				<b>AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting</b>
				<br><i>Preprint</i>. [<a href="https://arxiv.org/pdf/2505.18822" style="color:#9B0145;">arXiv</a>] [<a href="https://github.com/JoeYing1019/AdaCtrl" style="color:#9B0145;">code</a>]
			</li>
		        <br>
		    	<li>
				<b>Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers</b>
				<br><i>ðŸ”¥PreprintðŸ”¥</i>. [<a href="https://arxiv.org/abs/2506.23918" style="color:#9B0145;">arXiv</a>] [<a href="https://github.com/zhaochen0110/Awesome_Think_With_Images" style="color:#9B0145;">code</a>]
			</li>
        </ol>

	
		<details>
		<summary><font size="+1.5"><strong><I></I>Other Publications</I></strong></font></summary>
		<ul>
			<li>
				<b>Scaling Laws of Synthetic Data for Language Models</b>
				<br><i>COLM</i>, 2025 [<a href="https://arxiv.org/abs/2503.19551v2" style="color:#9B0145;">arXiv</a>]
			</li>
                        <br>
			<li>
				<b>R-Tuning: Instructing Large Language Models to Say â€˜I Donâ€™t Knowâ€™</b>
				<br><i>NAACL 2024 (Outstanding Paper)</i>. [<a href="" style="color:#9B0145;">arXiv</a>]
			</li>
		        <br>
		        <li>
				<b>Word Embeddings Are Steers for Language Models</b>
				<br><i>ACL 2024 (Outstanding Paper)</i>. [<a href="" style="color:#9B0145;">arXiv</a>]
			</li>
		        <br>
		        <li>
				<b>Tool Learning with Foundation Models</b>
				<br><i>ACM Computing Survey</i>. [<a href="" style="color:#9B0145;">arXiv</a>]
			</li>
		        <br>
		        <li>
				<b>COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation</b>
				<br><i>NAACL 2021 (Best Demo)</i>. [<a href="" style="color:#9B0145;">arXiv</a>]
			</li>
		</ul>
	</details>
	</div>
	
    <!-- </div> -->
	<hr style="color:rgb(218, 218, 218);">
		</div>
		<br>
		<br>
      </div>
    </div>
</div>
</body>
</html>
